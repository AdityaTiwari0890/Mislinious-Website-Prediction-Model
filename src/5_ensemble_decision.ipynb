{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b22f78",
   "metadata": {},
   "source": [
    "# Ensemble Decision Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f39093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trusted domains\n",
    "trusted_domains = [\n",
    "    'gov.in', 'edu.in', 'ac.in', 'lpu.in', 'nic.in', 'google.com', 'youtube.com', 'facebook.com', 'wikipedia.org', 'amazon.com'\n",
    "]\n",
    "\n",
    "def is_trusted_domain(url):\n",
    "    try:\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "        return any(td in domain for td in trusted_domains)\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860cc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Threat Intelligence APIs\n",
    "def check_urlhaus(url):\n",
    "    # Mock: check if url contains known malicious patterns\n",
    "    malicious_patterns = ['phishing', 'malware', 'br-icloud.com.br']\n",
    "    return any(pattern in url for pattern in malicious_patterns)\n",
    "\n",
    "def check_phishtank(url):\n",
    "    # Mock: similar\n",
    "    return 'phish' in url or 'fake' in url\n",
    "\n",
    "def check_google_safe_browsing(url):\n",
    "    # Mock\n",
    "    return 'malicious' in url\n",
    "\n",
    "def threat_api_check(url):\n",
    "    return check_urlhaus(url) or check_phishtank(url) or check_google_safe_browsing(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL validity check\n",
    "def is_url_alive(url):\n",
    "    try:\n",
    "        response = requests.head(url, timeout=5)\n",
    "        return response.status_code < 400\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c85828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction (same as in feature engineering)\n",
    "def extract_features(url):\n",
    "    features = {}\n",
    "    \n",
    "    features['url_length'] = len(url)\n",
    "    features['num_digits'] = sum(c.isdigit() for c in url)\n",
    "    special_chars = ['@', '?', '-', '_', '.', '/', '=', '&', '%', '+', '$', '#', '!', '*', '(', ')', '[', ']', '{', '}', '|', '\\\\', ':', ';', '\"', \"'\", '<', '>', ',']\n",
    "    features['num_special'] = sum(url.count(char) for char in special_chars)\n",
    "    \n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        import ipaddress\n",
    "        ipaddress.ip_address(domain)\n",
    "        features['has_ip'] = 1\n",
    "    except:\n",
    "        features['has_ip'] = 0\n",
    "    \n",
    "    features['path_length'] = len(parsed.path)\n",
    "    features['domain_length'] = len(domain)\n",
    "    features['num_subdomains'] = domain.count('.') - 1 if domain else 0\n",
    "    \n",
    "    suspicious_words = ['login', 'verify', 'secure', 'account', 'update', 'bank', 'paypal', 'free', 'win', 'password']\n",
    "    features['has_suspicious_words'] = int(any(word in url.lower() for word in suspicious_words))\n",
    "    \n",
    "    def entropy(s):\n",
    "        from collections import Counter\n",
    "        p, lns = Counter(s), float(len(s))\n",
    "        return -sum(count/lns * np.log2(count/lns) for count in p.values()) if lns > 0 else 0\n",
    "    features['entropy'] = entropy(url)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "log_reg = joblib.load('../models/logistic_regression.pkl')\n",
    "nb = joblib.load('../models/naive_bayes.pkl')\n",
    "rf = joblib.load('../models/random_forest.pkl')\n",
    "iso = joblib.load('../models/isolation_forest.pkl')\n",
    "\n",
    "print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205800e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final decision function\n",
    "def predict_url(url):\n",
    "    # Normalize\n",
    "    url = url.lower().strip()\n",
    "    \n",
    "    # 1. Trusted domain check\n",
    "    if is_trusted_domain(url):\n",
    "        return \"SAFE\", \"Trusted domain\"\n",
    "    \n",
    "    # 2. URL validity\n",
    "    if not is_url_alive(url):\n",
    "        return \"SUSPICIOUS\", \"URL not reachable\"\n",
    "    \n",
    "    # 3. Threat API check\n",
    "    if threat_api_check(url):\n",
    "        return \"MALICIOUS\", \"Flagged by threat intelligence\"\n",
    "    \n",
    "    # 4. Extract features\n",
    "    features = extract_features(url)\n",
    "    X = pd.DataFrame([features])\n",
    "    feature_cols = ['url_length', 'num_digits', 'num_special', 'has_ip', 'path_length', 'domain_length', 'num_subdomains', 'has_suspicious_words', 'entropy']\n",
    "    X = X[feature_cols]\n",
    "    \n",
    "    # 5. Supervised predictions\n",
    "    pred_log = log_reg.predict(X)[0]\n",
    "    pred_nb = nb.predict(X)[0]\n",
    "    pred_rf = rf.predict(X)[0]\n",
    "    \n",
    "    votes = [pred_log, pred_nb, pred_rf]\n",
    "    majority_vote = 1 if sum(votes) >= 2 else 0\n",
    "    \n",
    "    # 6. Isolation Forest\n",
    "    iso_pred = iso.predict(X)[0]\n",
    "    anomaly = 1 if iso_pred == -1 else 0\n",
    "    \n",
    "    # Final rules\n",
    "    if majority_vote == 1 or anomaly == 1:\n",
    "        return \"MALICIOUS\", f\"ML majority: {sum(votes)}/3, Anomaly: {anomaly}\"\n",
    "    else:\n",
    "        return \"SAFE\", \"No flags detected\"\n",
    "\n",
    "# Test\n",
    "test_urls = [\"https://www.google.com\", \"https://br-icloud.com.br\", \"https://gov.in\"]\n",
    "for url in test_urls:\n",
    "    result, reason = predict_url(url)\n",
    "    print(f\"{url}: {result} - {reason}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
